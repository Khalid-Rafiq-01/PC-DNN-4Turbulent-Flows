{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Importing the Necessary Packages"
      ],
      "metadata": {
        "id": "xKDKVMMOUze3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pnDkmurmHflz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10dc8eb8-8658-45ea-df91-6b4159b2a46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fei2SghErlpe"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "V0qlSdiYUr1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the Flow-Field Channel"
      ],
      "metadata": {
        "id": "-2kNRMlNUwHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Flow-Field Channel\n",
        "flow_field = np.ones((128,256), dtype = np.uint8)\n",
        "\n",
        "# Changing the left input side\n",
        "flow_field[:,0] = 3\n",
        "# Changing the right output side\n",
        "flow_field[:,-1] = 4\n",
        "# Changing the top layer\n",
        "flow_field[0,:] = 2\n",
        "# Changing the bottom layer\n",
        "flow_field[-1,:] = 2"
      ],
      "metadata": {
        "id": "UrAr4oilUus9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the Normalization values from training data and denormalizing the Ouptut Predictions using these central tendencies!\n",
        "mean_u = 0.075003795\n",
        "mean_v = -0.000036\n",
        "mean_p = 0.004301\n",
        "\n",
        "std_dev_u = 0.04605\n",
        "std_dev_v = 0.013812\n",
        "std_dev_p = 0.007917"
      ],
      "metadata": {
        "id": "ZBbK_00GU83B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the NS-Loss again"
      ],
      "metadata": {
        "id": "KztAxzJmVFDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nvs_loss(y_pred, rho=10, nu=0.0001): #arbitary rho and nu(Later use values of air)\n",
        "      u,v,p = tf.split(y_pred, 3, axis=3)\n",
        "\n",
        "  #First order derivative\n",
        "      du_dx, du_dy = tf.image.image_gradients(u) # tf.image.image_gradients returns a tuple containing two tensors: u-grad along the x dir and u-grad along the y dir\n",
        "      dv_dx, dv_dy = tf.image.image_gradients(v)\n",
        "      dp_dx, dp_dy = tf.image.image_gradients(p)\n",
        "\n",
        "  #Second order derivatives\n",
        "      du_dx2, du_dydx = tf.image.image_gradients(du_dx) # du_dydx will be unused\n",
        "      du_dxdy, du_dy2 = tf.image.image_gradients(du_dy) # du_dxdy will be unused\n",
        "\n",
        "      dv_dx2, dv_dydx = tf.image.image_gradients(dv_dx)\n",
        "      dv_dxdy, dv_dy2 = tf.image.image_gradients(dv_dy)\n",
        "\n",
        "  #Momentum equation\n",
        "      er1_tensor = tf.math.multiply(u, du_dx) + tf.math.multiply(v, du_dy) + 1.0*dp_dx/rho - nu*(du_dx2 + du_dy2)\n",
        "      er2_tensor = tf.math.multiply(u, dv_dx) + tf.math.multiply(v, dv_dy) + 1.0*dp_dy/rho - nu*(dv_dx2 + dv_dy2)\n",
        "\n",
        "  # # #Continuity equation\n",
        "      er3_tensor = du_dx + dv_dy\n",
        "\n",
        "      er1 = tf.reduce_mean(er1_tensor)\n",
        "      er2 = tf.reduce_mean(er2_tensor)\n",
        "      er3 = tf.reduce_mean(er3_tensor)\n",
        "\n",
        "      return  er1*er1 + er2*er2 + er3*er3\n",
        "\n",
        " # Initiating the Loss Function-\n",
        "def custom_loss(y_true, y_pred):\n",
        "  nv_loss = nvs_loss(y_pred)\n",
        "  mse_loss = tf.reduce_mean(tf.square(y_true-y_pred))  # Try mse loss function here\n",
        "  return mse_loss + nv_loss"
      ],
      "metadata": {
        "id": "p4-vTL9jVAr8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Developing a Colorizing Function!"
      ],
      "metadata": {
        "id": "GCCvP4rmVWuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib\n",
        "def colorize(value, vmin=None, vmax=None, cmap='gray_r', invalid_val=-99, invalid_mask=None, background_color=(128, 128, 128, 255), gamma_corrected=False, value_transform=None):\n",
        "    \"\"\"Converts a depth map to a color image.\n",
        "\n",
        "    Args:\n",
        "        value (torch.Tensor, numpy.ndarry): Input depth map. Shape: (H, W) or (1, H, W) or (1, 1, H, W). All singular dimensions are squeezed\n",
        "        vmin (float, optional): vmin-valued entries are mapped to start color of cmap. If None, value.min() is used. Defaults to None.\n",
        "        vmax (float, optional):  vmax-valued entries are mapped to end color of cmap. If None, value.max() is used. Defaults to None.\n",
        "        cmap (str, optional): matplotlib colormap to use. Defaults to 'magma_r'.\n",
        "        invalid_val (int, optional): Specifies value of invalid pixels that should be colored as 'background_color'. Defaults to -99.\n",
        "        invalid_mask (numpy.ndarray, optional): Boolean mask for invalid regions. Defaults to None.\n",
        "        background_color (tuple[int], optional): 4-tuple RGB color to give to invalid pixels. Defaults to (128, 128, 128, 255).\n",
        "        gamma_corrected (bool, optional): Apply gamma correction to colored image. Defaults to False.\n",
        "        value_transform (Callable, optional): Apply transform function to valid pixels before coloring. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray, dtype - uint8: Colored depth map. Shape: (H, W, 4)\n",
        "    \"\"\"\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        value = value.detach().cpu().numpy()\n",
        "\n",
        "    value = value.squeeze()\n",
        "    if invalid_mask is None:\n",
        "        invalid_mask = value == invalid_val\n",
        "    mask = np.logical_not(invalid_mask)\n",
        "\n",
        "    # normalize\n",
        "    # vmin = np.percentile(value[mask],2) if vmin is None else vmin\n",
        "    # vmax = np.percentile(value[mask],85) if vmax is None else vmax\n",
        "    vmin = np.min(value[mask]) if vmin is None else vmin\n",
        "    vmax = np.max(value[mask]) if vmax is None else vmax\n",
        "    if vmin != vmax:\n",
        "        value = (value - vmin) / (vmax - vmin)  # vmin..vmax\n",
        "    else:\n",
        "        # Avoid 0-division\n",
        "        value = value * 0.\n",
        "\n",
        "    # squeeze last dim if it exists\n",
        "    # grey out the invalid values\n",
        "\n",
        "    value[invalid_mask] = np.nan\n",
        "    cmapper = matplotlib.cm.get_cmap(cmap)\n",
        "    if value_transform:\n",
        "        value = value_transform(value)\n",
        "        # value = value / value.max()\n",
        "    value = cmapper(value, bytes=True)  # (nxmx4)\n",
        "\n",
        "    # img = value[:, :, :]\n",
        "    img = value[...]\n",
        "    img[invalid_mask] = background_color\n",
        "\n",
        "    #     return img.transpose((2, 0, 1))\n",
        "    if gamma_corrected:\n",
        "        # gamma correction\n",
        "        img = img / 255\n",
        "        img = np.power(img, 2.2)\n",
        "        img = img * 255\n",
        "        img = img.astype(np.uint8)\n",
        "    return img"
      ],
      "metadata": {
        "id": "ShRQ7v3GVUQp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Image - Processing"
      ],
      "metadata": {
        "id": "dWb6ef_yVcXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing Image pre-processing to fill the inside of the sketch with solid object!\n",
        "\n",
        "def img_preprocess(image, h, w):\n",
        "      # Convert the drawn image to grayscale\n",
        "    img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Threshold the grayscale image to create a binary image\n",
        "    _, binary_img = cv2.threshold(img_gray, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Perform flood fill starting from a point inside the shape. Fill the inside with pixel value 0\n",
        "    seed_point = (int(h/2), int(w/2))\n",
        "    retval, flooded_image, mask, rect = cv2.floodFill(binary_img, None, seed_point, 0)\n",
        "    flooded_image = (flooded_image/255).astype(np.uint8)\n",
        "    return flooded_image\n",
        "\n",
        "# Stitching the above patch to the center of channel flow\n",
        "def patch_stiching(flooded_image, h, w, x0, y0): # ((x0, y0) = center of channel,  (w1, h1) = height and width of patch)\n",
        "    flow_field_updated = np.copy(flow_field)\n",
        "    print('flow field updated - ', flow_field_updated[:,-1])\n",
        "    flow_field_updated[int(x0-w/2):int(x0+w/2),int(y0-h/2):int(y0+h/2)] = flooded_image\n",
        "\n",
        "\n",
        "    # flow_field_updated is the main thing that we will use to make our predictions on -\n",
        "    test_img = np.expand_dims(flow_field_updated, axis = 0)\n",
        "    test_img = np.expand_dims(test_img, axis = 3) # Shape of test_img = (1, 128, 256)\n",
        "    return test_img"
      ],
      "metadata": {
        "id": "5lvDzVPmVbRw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define grid points\n",
        "x_points = np.linspace(0, 255, 256)\n",
        "y_points = np.linspace(0, 127, 128)\n",
        "X, Y = np.meshgrid(x_points, y_points)\n",
        "\n",
        "#Defining the Quiver plot\n",
        "def return_quiver_plot(u, v):\n",
        "    velocity = np.sqrt(u**2 + v**2)\n",
        "    ax = plt.subplot()\n",
        "    ax.imshow(velocity, origin = 'lower', extent = (0,256, 0,128), cmap = 'gray')\n",
        "    q = ax.quiver(X[5::8,5::8], Y[5::8,5::8], u[5::8,5::8], u[5::8,5::8], pivot = 'middle', color = 'red', scale = 2)\n",
        "    # ax.quiverkey(q, X=0.9, Y=1.05, U=2,\n",
        "    #             label='m/s', labelpos='E')\n",
        "    # plt.title(\"Velocity distribution\")\n",
        "    # plt.show()\n",
        "    return q\n",
        "\n",
        "def squeeze_function(img):\n",
        "  img = np.squeeze(img, axis = 0)\n",
        "  img = np.squeeze(img, axis = 2)\n",
        "  return img"
      ],
      "metadata": {
        "id": "qNOngQ5gVzgr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main Gradio Code Block"
      ],
      "metadata": {
        "id": "XSneFA8SV9T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a shape from the user on sketchpad and placing it inside the fluid flow -\n",
        "\n",
        "h, w = 48, 48 # patch_size in which the obstacle will be drawn\n",
        "x0, y0 = 64, 128 # (x0, y0) = center of channel\n",
        "\n",
        "def fill_shape_with_pixels(img): #img is taken by gradio as uint8\n",
        "    if img is None:\n",
        "        return np.zeros((h, w), dtype=np.uint8) # \"No input sketch\"\n",
        "# Calling the the flooded image function to fill inside the obstacle\n",
        "    flooded_image = img_preprocess(img, h, w)\n",
        "# Performing patch statching to put the obstacle at the required center position\n",
        "    test_img = patch_stiching(flooded_image, h, w, x0, y0)\n",
        "\n",
        "# Loading and Compiling the Model\n",
        "    model_path = \"/content/drive/MyDrive/Pinns_Loss_file.h5\"\n",
        "    model = load_model(model_path, compile = False)\n",
        "    model.compile(loss=custom_loss, optimizer=tf.keras.optimizers.AdamW(learning_rate = 0.0001), metrics=['mae', 'cosine_proximity'])\n",
        "\n",
        " # Making Model prediction from input sketch shape\n",
        "    prediction = model.predict(test_img) # (prediction.shape = (1, 128, 256, 3))\n",
        "    u_pred, v_pred, p_pred = np.split(prediction, 3, axis=3) # shape of u_pred, v_pred, p_pred = (1, 128, 256, 1)\n",
        "\n",
        " # De-Normalizing teh Data:\n",
        "    u_pred = ((u_pred*std_dev_u) + mean_u)\n",
        "    v_pred = ((v_pred*std_dev_v) + mean_v)\n",
        "    p_pred = ((p_pred*std_dev_p) + mean_p)\n",
        "\n",
        " # Making test_img in shape required by zero_pixel_location\n",
        "    req_img = squeeze_function(test_img)\n",
        "\n",
        "# Storing the location of 0 pixel values\n",
        "    #req_img = req_img.astype(int)\n",
        "    zero_pixel_locations = np.argwhere(req_img == 0)\n",
        "\n",
        "# Reducing the dimensions-\n",
        "    u_profile = u_pred[0][:,:,0]  # shape of u profile to compatible shape (H, W) = (128, 256)\n",
        "    v_profile = v_pred[0][:,:,0]\n",
        "    p_profile = p_pred[0][:,:,0]\n",
        "    p_profile[p_profile>0.02] = 0.02\n",
        "    hist, bins = np.histogram(p_profile, bins=20)\n",
        "    print(hist)\n",
        "    print(bins)\n",
        "\n",
        "# Creating a copy of the above profiles-\n",
        "    u_profile_dash = np.copy(u_profile)\n",
        "    v_profile_dash = np.copy(v_profile)\n",
        "\n",
        "# Creating a copy of the above profiles-\n",
        "    u_profile_dash_1 = np.copy(u_profile)\n",
        "    v_profile_dash_1 = np.copy(v_profile)\n",
        "\n",
        "\n",
        "# Hollowing the obstacle out from the u and v plots. Origin of imae is lop left and origin of plot is top right\n",
        "    for y, x in zero_pixel_locations:\n",
        "      u_profile_dash[128 - y, x] = 0\n",
        "      v_profile_dash[128 - y, x] = 0\n",
        "      # will be used for image\n",
        "      u_profile_dash_1[y, x] = 0\n",
        "      v_profile_dash_1[y, x] = 0\n",
        "\n",
        "\n",
        "# Quiver Plot\n",
        "    quiver_plot = plt.figure(figsize = (14,6), edgecolor = \"gray\")\n",
        "    velocity = np.sqrt(u_profile_dash_1**2 + v_profile_dash_1**2)\n",
        "    ax = plt.subplot()\n",
        "    ax.imshow(velocity, cmap = 'gray', extent = (0,256, 0,128))\n",
        "    q = ax.quiver(X[5::7, 5::7], Y[5::7, 5::7], u_profile_dash[5::7, 5::7], v_profile_dash[5::7, 5::7], pivot = 'middle', color = 'red')\n",
        "    ax.quiverkey(q, X=0.9, Y=1.07, U=2,\n",
        "                label='m/s', labelpos='E')\n",
        "    plt.title(\"Velocity distribution\", fontsize = 11)\n",
        "    plt.xlabel(\"Length of Channel\", fontsize = 11)\n",
        "    plt.ylabel(\"Height of Channel\", fontsize = 11)\n",
        "\n",
        " # StreamLine Plot\n",
        "    streamline_plot = plt.figure(figsize = (14,6), edgecolor = \"gray\")\n",
        "    plt.streamplot(X, Y, u_profile_dash, v_profile_dash, density = 4)\n",
        "    plt.axis('scaled')\n",
        "    plt.title(\"Streamline Plot\", fontsize = 11)\n",
        "    plt.xlabel(\"Length of Channel\", fontsize = 11)\n",
        "    plt.ylabel(\"Height of Channel\", fontsize = 11)\n",
        "\n",
        "  # Colorize taken from ZoeDepth Model\n",
        "    u_colored = colorize(u_profile, cmap = 'jet')\n",
        "    #cbar_u = plt.colorbar(u_profile,fraction=0.025, pad=0.05)\n",
        "    v_colored = colorize(v_profile, cmap = 'jet')\n",
        "    #cbar_v = plt.colorbar(v_colored,fraction=0.025, pad=0.05)\n",
        "    p_colored = colorize(p_profile, cmap = 'jet')\n",
        "    #cbar_p = plt.colorbar(p_colored,fraction=0.025, pad=0.05)\n",
        "\n",
        "\n",
        "    return colorize(req_img, cmap = 'jet'), quiver_plot, streamline_plot, u_colored, v_colored, p_colored"
      ],
      "metadata": {
        "id": "aw8Oj8a7V7_u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing gr.Blocks()\n",
        "\n",
        "with gr.Blocks(theme=\"Taithrah/Minimal\") as demo:\n",
        "  gr.Markdown(\n",
        "    \"\"\"\n",
        "    # Channel Flow - Physics Constrained DNN for Predicting Mean Turbulent Flows\n",
        "    The App solves 2-D incompressible steady state NS equations for any given 2-D closed geometry. Geometry needs to be drawn around the center of the patch.\\n\n",
        "    It predicts the streamlines,horizontal & vertical velocity profiles and the pressure profiles using a hybrid loss function.\\n\n",
        "    Model Parameters (In SI Units) - Kinematic Viscosity = 0.0001, Input horizontal velocity = 0.075, Input vertical velocity = 0\n",
        "    \"\"\")\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      input_sketch = gr.Image(label = \"Draw any Obstacle contour around the patch center\",\n",
        "                        tool=\"sketch\", source=\"canvas\", shape=(h, w), brush_radius = 3)\n",
        "      Process_button = gr.Button(\"Process Flow Parameters\")\n",
        "\n",
        "    with gr.Column():\n",
        "      filled_channel = gr.Image(label = \"Drawn object inside a Channel of dimensions 128*256\", container = True)\n",
        "\n",
        "  with gr.Row():\n",
        "    quiver_plot = gr.Plot(label = \"Velocity Distribution Around The Obstacle\", scale = 2)\n",
        "\n",
        "  with gr.Row():\n",
        "    streamline_plot = gr.Plot(label = \"Stream Lines Around The Obstacle\", scale = 2)\n",
        "\n",
        "  with gr.Row():\n",
        "    u_image = gr.Image(label = \"Horizontal Velocity\")\n",
        "    v_image = gr.Image(label = \"Vertical Velocity\")\n",
        "    p_image = gr.Image(label = \"Pressure\")\n",
        "\n",
        "\n",
        "  Process_button.click(fn=fill_shape_with_pixels, inputs=input_sketch, outputs=[filled_channel, quiver_plot, streamline_plot, u_image, v_image, p_image])\n",
        "\n",
        "demo.launch(debug=True, server_name = \"0.0.0.0\", share = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "VqiCzDDHWHZf",
        "outputId": "b5c98be2-1b25-4ad9-f92e-d5aa666f8ca3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a282fc4d7768315821.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a282fc4d7768315821.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flow field updated -  [2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2]\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "[  15  149  777 7122 9054  709  647  845 5200 7176  711  242   75   36\n",
            "    3    2    1    1    1    2]\n",
            "[-4.97243786e-03 -3.72381601e-03 -2.47519417e-03 -1.22657220e-03\n",
            "  2.20496204e-05  1.27067149e-03  2.51929346e-03  3.76791530e-03\n",
            "  5.01653692e-03  6.26515877e-03  7.51378108e-03  8.76240246e-03\n",
            "  1.00110248e-02  1.12596462e-02  1.25082685e-02  1.37568898e-02\n",
            "  1.50055122e-02  1.62541345e-02  1.75027549e-02  1.87513772e-02\n",
            "  1.99999996e-02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-ca742561e654>:43: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
            "  cmapper = matplotlib.cm.get_cmap(cmap)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://a282fc4d7768315821.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gFQQNRmFWItD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SStIeKOyWIoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}